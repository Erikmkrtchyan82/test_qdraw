{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df9bee7-0e15-42d5-92df-f4dc9da36419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 08:31:23.857920: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import model as sketch_rnn_model\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import six\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83af8466-e58b-4aef-a03a-2f8cfcbf0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'data_dir',\n",
    "    'https://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep',\n",
    "    'The directory in which to find the dataset specified in model hparams. '\n",
    "    'If data_dir starts with \"http://\" or \"https://\", the file will be fetched '\n",
    "    'remotely.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'log_root', '/tmp/sketch_rnn/models/default',\n",
    "    'Directory to store model checkpoints, tensorboard.')\n",
    "tf.app.flags.DEFINE_boolean(\n",
    "    'resume_training', False,\n",
    "    'Set to true to load previous checkpoint')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'hparams', '',\n",
    "    'Pass in comma-separated key=value pairs such as '\n",
    "    '\\'save_every=40,decay_rate=0.99\\' '\n",
    "    '(no whitespace) to be read into the HParams object defined in model.py')\n",
    "\n",
    "PRETRAINED_MODELS_URL = ('http://download.magenta.tensorflow.org/models/'\n",
    "                         'sketch_rnn.zip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190b61ff-d306-40aa-bc7f-926f7aebeab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "  \"\"\"Closes the current default session and resets the graph.\"\"\"\n",
    "  sess = tf.get_default_session()\n",
    "  if sess:\n",
    "    sess.close()\n",
    "  tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfde9b05-e956-432b-a1c8-50f577a06d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_env(data_dir, model_dir):\n",
    "  \"\"\"Loads environment for inference mode, used in jupyter notebook.\"\"\"\n",
    "  model_params = sketch_rnn_model.get_default_hparams()\n",
    "  with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n",
    "    model_params.parse_json(f.read())\n",
    "  return load_dataset(data_dir, model_params, inference_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d785cb3-2337-49d0-9171-2ea76b1c9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_dir):\n",
    "  \"\"\"Loads model for inference mode, used in jupyter notebook.\"\"\"\n",
    "  model_params = sketch_rnn_model.get_default_hparams()\n",
    "  with tf.gfile.Open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n",
    "    model_params.parse_json(f.read())\n",
    "\n",
    "  model_params.batch_size = 1  # only sample one at a time\n",
    "  eval_model_params = sketch_rnn_model.copy_hparams(model_params)\n",
    "  eval_model_params.use_input_dropout = 0\n",
    "  eval_model_params.use_recurrent_dropout = 0\n",
    "  eval_model_params.use_output_dropout = 0\n",
    "  eval_model_params.is_training = 0\n",
    "  sample_model_params = sketch_rnn_model.copy_hparams(eval_model_params)\n",
    "  sample_model_params.max_seq_len = 1  # sample one point at a time\n",
    "  return [model_params, eval_model_params, sample_model_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08632f03-b442-4edb-ac32-2bb2c5bf0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pretrained_models(\n",
    "    models_root_dir='/tmp/sketch_rnn/models',\n",
    "    pretrained_models_url=PRETRAINED_MODELS_URL):\n",
    "  \"\"\"Download pretrained models to a temporary directory.\"\"\"\n",
    "  tf.gfile.MakeDirs(models_root_dir)\n",
    "  zip_path = os.path.join(\n",
    "      models_root_dir, os.path.basename(pretrained_models_url))\n",
    "  if os.path.isfile(zip_path):\n",
    "    tf.logging.info('%s already exists, using cached copy', zip_path)\n",
    "  else:\n",
    "    tf.logging.info('Downloading pretrained models from %s...',\n",
    "                    pretrained_models_url)\n",
    "    urlretrieve(pretrained_models_url, zip_path)\n",
    "    tf.logging.info('Download complete.')\n",
    "  tf.logging.info('Unzipping %s...', zip_path)\n",
    "  with zipfile.ZipFile(zip_path) as models_zip:\n",
    "    models_zip.extractall(models_root_dir)\n",
    "  tf.logging.info('Unzipping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e57e405-60b1-4d17-8ac5-23be16bd3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_dir, model_params, inference_mode=False):\n",
    "  \"\"\"Loads the .npz file, and splits the set into train/valid/test.\"\"\"\n",
    "\n",
    "  # normalizes the x and y columns using the training set.\n",
    "  # applies same scaling factor to valid and test set.\n",
    "\n",
    "  if isinstance(model_params.data_set, list):\n",
    "    datasets = model_params.data_set\n",
    "  else:\n",
    "    datasets = [model_params.data_set]\n",
    "\n",
    "  train_strokes = None\n",
    "  valid_strokes = None\n",
    "  test_strokes = None\n",
    "\n",
    "  for dataset in datasets:\n",
    "    if data_dir.startswith('http://') or data_dir.startswith('https://'):\n",
    "      data_filepath = '/'.join([data_dir, dataset])\n",
    "      tf.logging.info('Downloading %s', data_filepath)\n",
    "      response = requests.get(data_filepath)\n",
    "      data = np.load(six.BytesIO(response.content), encoding='latin1')\n",
    "    else:\n",
    "      data_filepath = os.path.join(data_dir, dataset)\n",
    "      data = np.load(data_filepath, encoding='latin1', allow_pickle=True)\n",
    "    tf.logging.info('Loaded {}/{}/{} from {}'.format(\n",
    "        len(data['train']), len(data['valid']), len(data['test']),\n",
    "        dataset))\n",
    "    if train_strokes is None:\n",
    "      train_strokes = data['train']\n",
    "      valid_strokes = data['valid']\n",
    "      test_strokes = data['test']\n",
    "    else:\n",
    "      train_strokes = np.concatenate((train_strokes, data['train']))\n",
    "      valid_strokes = np.concatenate((valid_strokes, data['valid']))\n",
    "      test_strokes = np.concatenate((test_strokes, data['test']))\n",
    "\n",
    "  all_strokes = np.concatenate((train_strokes, valid_strokes, test_strokes))\n",
    "  num_points = 0\n",
    "  for stroke in all_strokes:\n",
    "    num_points += len(stroke)\n",
    "  avg_len = num_points / len(all_strokes)\n",
    "  tf.logging.info('Dataset combined: {} ({}/{}/{}), avg len {}'.format(\n",
    "      len(all_strokes), len(train_strokes), len(valid_strokes),\n",
    "      len(test_strokes), int(avg_len)))\n",
    "\n",
    "  # calculate the max strokes we need.\n",
    "  max_seq_len = utils.get_max_len(all_strokes)\n",
    "  # overwrite the hps with this calculation.\n",
    "  model_params.max_seq_len = max_seq_len\n",
    "\n",
    "  tf.logging.info('model_params.max_seq_len %i.', model_params.max_seq_len)\n",
    "\n",
    "  eval_model_params = sketch_rnn_model.copy_hparams(model_params)\n",
    "\n",
    "  eval_model_params.use_input_dropout = 0\n",
    "  eval_model_params.use_recurrent_dropout = 0\n",
    "  eval_model_params.use_output_dropout = 0\n",
    "  eval_model_params.is_training = 1\n",
    "\n",
    "  if inference_mode:\n",
    "    eval_model_params.batch_size = 1\n",
    "    eval_model_params.is_training = 0\n",
    "\n",
    "  sample_model_params = sketch_rnn_model.copy_hparams(eval_model_params)\n",
    "  sample_model_params.batch_size = 1  # only sample one at a time\n",
    "  sample_model_params.max_seq_len = 1  # sample one point at a time\n",
    "\n",
    "  train_set = utils.DataLoader(\n",
    "      train_strokes,\n",
    "      model_params.batch_size,\n",
    "      max_seq_length=model_params.max_seq_len,\n",
    "      random_scale_factor=model_params.random_scale_factor,\n",
    "      augment_stroke_prob=model_params.augment_stroke_prob)\n",
    "\n",
    "  normalizing_scale_factor = train_set.calculate_normalizing_scale_factor()\n",
    "  train_set.normalize(normalizing_scale_factor)\n",
    "\n",
    "  valid_set = utils.DataLoader(\n",
    "      valid_strokes,\n",
    "      eval_model_params.batch_size,\n",
    "      max_seq_length=eval_model_params.max_seq_len,\n",
    "      random_scale_factor=0.0,\n",
    "      augment_stroke_prob=0.0)\n",
    "  valid_set.normalize(normalizing_scale_factor)\n",
    "\n",
    "  test_set = utils.DataLoader(\n",
    "      test_strokes,\n",
    "      eval_model_params.batch_size,\n",
    "      max_seq_length=eval_model_params.max_seq_len,\n",
    "      random_scale_factor=0.0,\n",
    "      augment_stroke_prob=0.0)\n",
    "  test_set.normalize(normalizing_scale_factor)\n",
    "\n",
    "  tf.logging.info('normalizing_scale_factor %4.4f.', normalizing_scale_factor)\n",
    "\n",
    "  result = [\n",
    "      train_set, valid_set, test_set, model_params, eval_model_params,\n",
    "      sample_model_params\n",
    "  ]\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c101012b-8e0b-49cd-8e16-f5c83e7439cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(sess, model, data_set):\n",
    "  \"\"\"Returns the average weighted cost, reconstruction cost and KL cost.\"\"\"\n",
    "  total_cost = 0.0\n",
    "  total_r_cost = 0.0\n",
    "  total_kl_cost = 0.0\n",
    "  for batch in range(data_set.num_batches):\n",
    "    unused_orig_x, x, s = data_set.get_batch(batch)\n",
    "    feed = {model.input_data: x, model.sequence_lengths: s}\n",
    "    (cost, r_cost,\n",
    "     kl_cost) = sess.run([model.cost, model.r_cost, model.kl_cost], feed)\n",
    "    total_cost += cost\n",
    "    total_r_cost += r_cost\n",
    "    total_kl_cost += kl_cost\n",
    "\n",
    "  total_cost /= (data_set.num_batches)\n",
    "  total_r_cost /= (data_set.num_batches)\n",
    "  total_kl_cost /= (data_set.num_batches)\n",
    "  return (total_cost, total_r_cost, total_kl_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e979960d-3fcb-4e0e-ae83-b115d137e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(sess, checkpoint_path):\n",
    "  saver = tf.train.Saver(tf.global_variables())\n",
    "  ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "  tf.logging.info('Loading model %s.', ckpt.model_checkpoint_path)\n",
    "  saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e2053f5-bf73-413b-80e2-fb1b8826f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(sess, model_save_path, global_step):\n",
    "  saver = tf.train.Saver(tf.global_variables())\n",
    "  checkpoint_path = os.path.join(model_save_path, 'vector')\n",
    "  tf.logging.info('saving model %s.', checkpoint_path)\n",
    "  tf.logging.info('global_step %i.', global_step)\n",
    "  saver.save(sess, checkpoint_path, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff4817fa-0d1d-47d8-8713-3de51a77e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, model, eval_model, train_set, valid_set, test_set):\n",
    "  \"\"\"Train a sketch-rnn model.\"\"\"\n",
    "  # Setup summary writer.\n",
    "  summary_writer = tf.summary.FileWriter(FLAGS.log_root)\n",
    "\n",
    "  # Calculate trainable params.\n",
    "  t_vars = tf.trainable_variables()\n",
    "  count_t_vars = 0\n",
    "  for var in t_vars:\n",
    "    num_param = np.prod(var.get_shape().as_list())\n",
    "    count_t_vars += num_param\n",
    "    tf.logging.info('%s %s %i', var.name, str(var.get_shape()), num_param)\n",
    "  tf.logging.info('Total trainable variables %i.', count_t_vars)\n",
    "  model_summ = tf.summary.Summary()\n",
    "  model_summ.value.add(\n",
    "      tag='Num_Trainable_Params', simple_value=float(count_t_vars))\n",
    "  summary_writer.add_summary(model_summ, 0)\n",
    "  summary_writer.flush()\n",
    "\n",
    "  # setup eval stats\n",
    "  best_valid_cost = 100000000.0  # set a large init value\n",
    "  valid_cost = 0.0\n",
    "\n",
    "  # main train loop\n",
    "\n",
    "  hps = model.hps\n",
    "  start = time.time()\n",
    "\n",
    "  for _ in range(hps.num_steps):\n",
    "\n",
    "    step = sess.run(model.global_step)\n",
    "\n",
    "    curr_learning_rate = ((hps.learning_rate - hps.min_learning_rate) *\n",
    "                          (hps.decay_rate)**step + hps.min_learning_rate)\n",
    "    curr_kl_weight = (hps.kl_weight - (hps.kl_weight - hps.kl_weight_start) *\n",
    "                      (hps.kl_decay_rate)**step)\n",
    "\n",
    "    _, x, s = train_set.random_batch()\n",
    "    feed = {\n",
    "        model.input_data: x,\n",
    "        model.sequence_lengths: s,\n",
    "        model.lr: curr_learning_rate,\n",
    "        model.kl_weight: curr_kl_weight\n",
    "    }\n",
    "\n",
    "    (train_cost, r_cost, kl_cost, _, train_step, _) = sess.run([\n",
    "        model.cost, model.r_cost, model.kl_cost, model.final_state,\n",
    "        model.global_step, model.train_op\n",
    "    ], feed)\n",
    "\n",
    "    if step % 20 == 0 and step > 0:\n",
    "\n",
    "      end = time.time()\n",
    "      time_taken = end - start\n",
    "\n",
    "      cost_summ = tf.summary.Summary()\n",
    "      cost_summ.value.add(tag='Train_Cost', simple_value=float(train_cost))\n",
    "      reconstr_summ = tf.summary.Summary()\n",
    "      reconstr_summ.value.add(\n",
    "          tag='Train_Reconstr_Cost', simple_value=float(r_cost))\n",
    "      kl_summ = tf.summary.Summary()\n",
    "      kl_summ.value.add(tag='Train_KL_Cost', simple_value=float(kl_cost))\n",
    "      lr_summ = tf.summary.Summary()\n",
    "      lr_summ.value.add(\n",
    "          tag='Learning_Rate', simple_value=float(curr_learning_rate))\n",
    "      kl_weight_summ = tf.summary.Summary()\n",
    "      kl_weight_summ.value.add(\n",
    "          tag='KL_Weight', simple_value=float(curr_kl_weight))\n",
    "      time_summ = tf.summary.Summary()\n",
    "      time_summ.value.add(\n",
    "          tag='Time_Taken_Train', simple_value=float(time_taken))\n",
    "\n",
    "      output_format = ('step: %d, lr: %.6f, klw: %0.4f, cost: %.4f, '\n",
    "                       'recon: %.4f, kl: %.4f, train_time_taken: %.4f')\n",
    "      output_values = (step, curr_learning_rate, curr_kl_weight, train_cost,\n",
    "                       r_cost, kl_cost, time_taken)\n",
    "      output_log = output_format % output_values\n",
    "\n",
    "      tf.logging.info(output_log)\n",
    "\n",
    "      summary_writer.add_summary(cost_summ, train_step)\n",
    "      summary_writer.add_summary(reconstr_summ, train_step)\n",
    "      summary_writer.add_summary(kl_summ, train_step)\n",
    "      summary_writer.add_summary(lr_summ, train_step)\n",
    "      summary_writer.add_summary(kl_weight_summ, train_step)\n",
    "      summary_writer.add_summary(time_summ, train_step)\n",
    "      summary_writer.flush()\n",
    "      start = time.time()\n",
    "\n",
    "    if step % hps.save_every == 0 and step > 0:\n",
    "\n",
    "      (valid_cost, valid_r_cost, valid_kl_cost) = evaluate_model(\n",
    "          sess, eval_model, valid_set)\n",
    "\n",
    "      end = time.time()\n",
    "      time_taken_valid = end - start\n",
    "      start = time.time()\n",
    "\n",
    "      valid_cost_summ = tf.summary.Summary()\n",
    "      valid_cost_summ.value.add(\n",
    "          tag='Valid_Cost', simple_value=float(valid_cost))\n",
    "      valid_reconstr_summ = tf.summary.Summary()\n",
    "      valid_reconstr_summ.value.add(\n",
    "          tag='Valid_Reconstr_Cost', simple_value=float(valid_r_cost))\n",
    "      valid_kl_summ = tf.summary.Summary()\n",
    "      valid_kl_summ.value.add(\n",
    "          tag='Valid_KL_Cost', simple_value=float(valid_kl_cost))\n",
    "      valid_time_summ = tf.summary.Summary()\n",
    "      valid_time_summ.value.add(\n",
    "          tag='Time_Taken_Valid', simple_value=float(time_taken_valid))\n",
    "\n",
    "      output_format = ('best_valid_cost: %0.4f, valid_cost: %.4f, valid_recon: '\n",
    "                       '%.4f, valid_kl: %.4f, valid_time_taken: %.4f')\n",
    "      output_values = (min(best_valid_cost, valid_cost), valid_cost,\n",
    "                       valid_r_cost, valid_kl_cost, time_taken_valid)\n",
    "      output_log = output_format % output_values\n",
    "\n",
    "      tf.logging.info(output_log)\n",
    "\n",
    "      summary_writer.add_summary(valid_cost_summ, train_step)\n",
    "      summary_writer.add_summary(valid_reconstr_summ, train_step)\n",
    "      summary_writer.add_summary(valid_kl_summ, train_step)\n",
    "      summary_writer.add_summary(valid_time_summ, train_step)\n",
    "      summary_writer.flush()\n",
    "\n",
    "      if valid_cost < best_valid_cost:\n",
    "        best_valid_cost = valid_cost\n",
    "\n",
    "        save_model(sess, FLAGS.log_root, step)\n",
    "\n",
    "        end = time.time()\n",
    "        time_taken_save = end - start\n",
    "        start = time.time()\n",
    "\n",
    "        tf.logging.info('time_taken_save %4.4f.', time_taken_save)\n",
    "\n",
    "        best_valid_cost_summ = tf.summary.Summary()\n",
    "        best_valid_cost_summ.value.add(\n",
    "            tag='Best_Valid_Cost', simple_value=float(best_valid_cost))\n",
    "\n",
    "        summary_writer.add_summary(best_valid_cost_summ, train_step)\n",
    "        summary_writer.flush()\n",
    "\n",
    "        (eval_cost, eval_r_cost, eval_kl_cost) = evaluate_model(\n",
    "            sess, eval_model, test_set)\n",
    "\n",
    "        end = time.time()\n",
    "        time_taken_eval = end - start\n",
    "        start = time.time()\n",
    "\n",
    "        eval_cost_summ = tf.summary.Summary()\n",
    "        eval_cost_summ.value.add(tag='Eval_Cost', simple_value=float(eval_cost))\n",
    "        eval_reconstr_summ = tf.summary.Summary()\n",
    "        eval_reconstr_summ.value.add(\n",
    "            tag='Eval_Reconstr_Cost', simple_value=float(eval_r_cost))\n",
    "        eval_kl_summ = tf.summary.Summary()\n",
    "        eval_kl_summ.value.add(\n",
    "            tag='Eval_KL_Cost', simple_value=float(eval_kl_cost))\n",
    "        eval_time_summ = tf.summary.Summary()\n",
    "        eval_time_summ.value.add(\n",
    "            tag='Time_Taken_Eval', simple_value=float(time_taken_eval))\n",
    "\n",
    "        output_format = ('eval_cost: %.4f, eval_recon: %.4f, '\n",
    "                         'eval_kl: %.4f, eval_time_taken: %.4f')\n",
    "        output_values = (eval_cost, eval_r_cost, eval_kl_cost, time_taken_eval)\n",
    "        output_log = output_format % output_values\n",
    "\n",
    "        tf.logging.info(output_log)\n",
    "\n",
    "        summary_writer.add_summary(eval_cost_summ, train_step)\n",
    "        summary_writer.add_summary(eval_reconstr_summ, train_step)\n",
    "        summary_writer.add_summary(eval_kl_summ, train_step)\n",
    "        summary_writer.add_summary(eval_time_summ, train_step)\n",
    "        summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e974eb5-b6cb-45dc-a8dd-6e2d187349a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model_params):\n",
    "  \"\"\"Train a sketch-rnn model.\"\"\"\n",
    "  np.set_printoptions(precision=8, edgeitems=6, linewidth=200, suppress=True)\n",
    "\n",
    "  tf.logging.info('sketch-rnn')\n",
    "  tf.logging.info('Hyperparams:')\n",
    "  tf.logging.info('Loading data files.')\n",
    "  datasets = load_dataset(FLAGS.data_dir, model_params)\n",
    "\n",
    "  train_set = datasets[0]\n",
    "  valid_set = datasets[1]\n",
    "  test_set = datasets[2]\n",
    "  model_params = datasets[3]\n",
    "  eval_model_params = datasets[4]\n",
    "\n",
    "  reset_graph()\n",
    "  model = sketch_rnn_model.Model(model_params)\n",
    "  eval_model = sketch_rnn_model.Model(eval_model_params, reuse=True)\n",
    "\n",
    "  sess = tf.InteractiveSession()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  if FLAGS.resume_training:\n",
    "    load_checkpoint(sess, FLAGS.log_root)\n",
    "\n",
    "  # Write config file to json file.\n",
    "  tf.gfile.MakeDirs(FLAGS.log_root)\n",
    "  with tf.gfile.Open(\n",
    "      os.path.join(FLAGS.log_root, 'model_config.json'), 'w') as f:\n",
    "    json.dump(list(model_params.values()), f, indent=True)\n",
    "\n",
    "  train(sess, model, eval_model, train_set, valid_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f19cbae-4d20-414d-a600-3133382d173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "  \"\"\"Load model params, save config file and start trainer.\"\"\"\n",
    "  model_params = sketch_rnn_model.get_default_hparams()\n",
    "  if FLAGS.hparams:\n",
    "    model_params.parse(FLAGS.hparams)\n",
    "  trainer(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf1df8e2-6a2d-4a64-aa1b-f5ff6ca6cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def console_entry_point():\n",
    "  tf.disable_v2_behavior()\n",
    "  tf.app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62b9c3b2-82ad-4108-a011-2a451ac875a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "INFO:tensorflow:sketch-rnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 08:33:48.055906: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-30 08:33:50.071275: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-30 08:33:50.071360: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0330 08:33:50.072031 140174461067264 2756574815.py:5] sketch-rnn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Hyperparams:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 08:33:50.073713 140174461067264 2756574815.py:6] Hyperparams:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading data files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 08:33:50.075577 140174461067264 2756574815.py:7] Loading data files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Downloading https://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep/aaron_sheep.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0330 08:33:50.076794 140174461067264 1531447645.py:19] Downloading https://github.com/hardmaru/sketch-rnn-datasets/raw/master/aaron_sheep/aaron_sheep.npz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconsole_entry_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m, in \u001b[0;36mconsole_entry_point\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconsole_entry_point\u001b[39m():\n\u001b[1;32m      2\u001b[0m   tf\u001b[38;5;241m.\u001b[39mdisable_v2_behavior()\n\u001b[0;32m----> 3\u001b[0m   \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/platform/app.py:36\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the program with an optional 'main' function and 'argv' list.\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m main \u001b[38;5;241m=\u001b[39m main \u001b[38;5;129;01mor\u001b[39;00m _sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmain\n\u001b[0;32m---> 36\u001b[0m \u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_parse_flags_tolerate_undef\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/absl/app.py:308\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    306\u001b[0m   callback()\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m   \u001b[43m_run_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UsageError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    310\u001b[0m   usage(shorthelp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detailed_error\u001b[38;5;241m=\u001b[39merror, exitcode\u001b[38;5;241m=\u001b[39merror\u001b[38;5;241m.\u001b[39mexitcode)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/absl/app.py:254\u001b[0m, in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    252\u001b[0m   sys\u001b[38;5;241m.\u001b[39mexit(profiler\u001b[38;5;241m.\u001b[39mruncall(main, argv))\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m   sys\u001b[38;5;241m.\u001b[39mexit(\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(unused_argv)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m FLAGS\u001b[38;5;241m.\u001b[39mhparams:\n\u001b[1;32m      5\u001b[0m   model_params\u001b[38;5;241m.\u001b[39mparse(FLAGS\u001b[38;5;241m.\u001b[39mhparams)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(model_params)\u001b[0m\n\u001b[1;32m      6\u001b[0m tf\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHyperparams:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m tf\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading data files.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m train_set \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m valid_set \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(data_dir, model_params, inference_mode)\u001b[0m\n\u001b[1;32m     23\u001b[0m   data_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, dataset)\n\u001b[1;32m     24\u001b[0m   data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(data_filepath, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m tf\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m), \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     27\u001b[0m     dataset))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_strokes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m   train_strokes \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/npyio.py:256\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/format.py:795\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[0;32m--> 795\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject arrays cannot be loaded when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pickle_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m         pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "console_entry_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09a737-0a4e-4202-a569-30f420c0d639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
